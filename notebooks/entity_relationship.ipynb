{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd23886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"MyDocs\")\n",
    "\n",
    "OPENAI_API_KEY_CHAT = os.getenv(\"OPENAI_API_KEY_CHAT\")  # chat key (not embed)\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")  \n",
    "\n",
    "assert WEAVIATE_URL and WEAVIATE_API_KEY, \"Weaviate env vars missing\"\n",
    "assert OPENAI_API_KEY_CHAT, \"OPENAI_API_KEY_CHAT missing\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY_CHAT)\n",
    "\n",
    "# Weaviate REST / GraphQL headers\n",
    "weaviate_headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {WEAVIATE_API_KEY}\",\n",
    "}\n",
    "\n",
    "# Pagination / data volume\n",
    "PAGE_LIMIT = 500        # objects per GraphQL page\n",
    "MAX_CHUNKS = 2000       # set to None for full ~50k, start small for dev\n",
    "\n",
    "# Entity extraction settings\n",
    "ENTITY_TYPES = [\"PERSON\", \"ORG\", \"LOCATION\", \"EVENT\", \"WORK\", \"DATE\", \"OTHER\"]\n",
    "MAX_CHARS_PER_CHUNK = 4000   # safeguard, your chunks are small anyway\n",
    "\n",
    "# OpenAI throughput control (very rough)\n",
    "CHUNK_BATCH = 100             # chunks per loop before a short sleep\n",
    "SLEEP_SECONDS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9445b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_page(cursor: str = None, limit: int = PAGE_LIMIT) -> List[Dict[str, Any]]: \n",
    "    after_clause = f'after: \"{cursor}\"' if cursor else \"\"\n",
    "    query = {\n",
    "        \"query\": f\"\"\"\n",
    "        {{\n",
    "          Get {{\n",
    "            {COLLECTION_NAME}(\n",
    "              limit: {limit}\n",
    "              {after_clause}\n",
    "            ) {{\n",
    "              chunk_id\n",
    "              doc_id\n",
    "              text\n",
    "              _additional {{\n",
    "                id\n",
    "              }}\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    }\n",
    "    resp = requests.post(\n",
    "        f\"{WEAVIATE_URL}/v1/graphql\",\n",
    "        json=query,\n",
    "        headers=weaviate_headers,\n",
    "        timeout=60,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if \"errors\" in data:\n",
    "        raise RuntimeError(data[\"errors\"])\n",
    "    return data[\"data\"][\"Get\"][COLLECTION_NAME]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03c3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 500 chunks...\n",
      "Fetched 1000 chunks...\n",
      "Fetched 1500 chunks...\n",
      "Fetched 2000 chunks...\n",
      "Reached MAX_CHUNKS=2000, stopping.\n",
      "Total chunks fetched: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      chunk_id  doc_id                                               text  \\\n",
       " 0  016697_c937  016697  0024-001 3181903 0 3181903 RES N 50-43-43-10-1...   \n",
       " 1  023731_c153  023731   scholars. But, in a way, he was very wrong. Y...   \n",
       " 2  027333_c015  027333  5 Is Read: Yes Is Invitation: No GUID: 734F669...   \n",
       " 3  025231_c005  025231  -mail message is subject to the Dubai World Gr...   \n",
       " 4  017088_c153  017088   for a return of McCarthyism or for military h...   \n",
       " \n",
       "                                     _id  \n",
       " 0  000203a2-f584-43f1-8527-e167b0bf8c6e  \n",
       " 1  00046a59-5d95-4bdd-9bef-2acc2feead61  \n",
       " 2  0008b585-596a-42b9-8d0a-9be00a32b994  \n",
       " 3  00091260-d45d-4f44-bb4e-8100c280de0e  \n",
       " 4  00099043-8c80-4015-8d4c-6912225c5d60  ,\n",
       " (2000, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull chunks into a DataFrame\n",
    "all_rows = []\n",
    "cursor = None\n",
    "\n",
    "while True:\n",
    "    rows = fetch_page(cursor, PAGE_LIMIT)\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    all_rows.extend(rows)\n",
    "    cursor = rows[-1][\"_additional\"][\"id\"]\n",
    "    print(f\"Fetched {len(all_rows)} chunks...\")\n",
    "\n",
    "    if MAX_CHUNKS is not None and len(all_rows) >= MAX_CHUNKS:\n",
    "        all_rows = all_rows[:MAX_CHUNKS]\n",
    "        print(f\"Reached MAX_CHUNKS={MAX_CHUNKS}, stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"Total chunks fetched:\", len(all_rows))\n",
    "\n",
    "records = []\n",
    "for r in all_rows:\n",
    "    records.append(\n",
    "        {\n",
    "            \"chunk_id\": r.get(\"chunk_id\"),\n",
    "            \"doc_id\": r.get(\"doc_id\"),\n",
    "            \"text\": r.get(\"text\") or \"\",\n",
    "            \"_id\": r[\"_additional\"][\"id\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head(), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_TYPES = [\n",
    "    \"PERSON\",\n",
    "    \"ORG\",\n",
    "    \"LOCATION\",\n",
    "    \"EVENT\",\n",
    "    \"WORK\",\n",
    "    \"DATE\",\n",
    "    \"OTHER\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "some prompt here...\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities_for_chunk(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract entities + relations using OpenAI with strict constraints.\n",
    "    The system prompt prevents hallucination and external inference.\n",
    "    \"\"\"\n",
    "    text = text[:MAX_CHARS_PER_CHUNK]\n",
    "\n",
    "    user_content = (\n",
    "        \"Extract entities and relationships from this text. \"\n",
    "        \"Stay strictly within what the text explicitly states.\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\n\"\n",
    "        \"Respond ONLY with valid JSON.\"\n",
    "    )\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    parsed = json.loads(resp.choices[0].message.content)\n",
    "    return {\n",
    "        \"entities\": parsed.get(\"entities\", []) or [],\n",
    "        \"relations\": parsed.get(\"relations\", []) or [],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1180074-3188-4378-acac-3115d43a8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Configure context usage\n",
    "MODEL_CTX = 400_000                    # GPT-5 nano context window\n",
    "TARGET_FRACTION = 0.7                  # use 70 percent of context\n",
    "MAX_INPUT_TOKENS_PER_REQ = int(MODEL_CTX * TARGET_FRACTION)\n",
    "\n",
    "# Hard cap on number of chunks per request so JSON stays manageable\n",
    "MAX_ITEMS_PER_BATCH = 200\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def iter_token_batches(df_subset, max_tokens=MAX_INPUT_TOKENS_PER_REQ, max_items=MAX_ITEMS_PER_BATCH):\n",
    "    \"\"\"\n",
    "    Yield lists of rows (as namedtuples) so that the combined input tokens\n",
    "    stay under max_tokens and we never exceed max_items per batch.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    token_count = 0\n",
    "\n",
    "    # Rough overhead per item in the JSON wrapper and prompt text\n",
    "    PER_ITEM_OVERHEAD = 20\n",
    "    BASE_OVERHEAD = 300   # prompt, instructions, JSON scaffolding\n",
    "\n",
    "    for row in df_subset.itertuples(index=False):\n",
    "        text = row.text or \"\"\n",
    "        n_tokens = len(encoding.encode(text))\n",
    "        needed = n_tokens + PER_ITEM_OVERHEAD\n",
    "\n",
    "        # If adding this item would exceed budget or item cap, emit current batch\n",
    "        if batch and (token_count + needed + BASE_OVERHEAD > max_tokens or len(batch) >= max_items):\n",
    "            yield batch\n",
    "            batch = []\n",
    "            token_count = 0\n",
    "\n",
    "        batch.append(row)\n",
    "        token_count += needed\n",
    "\n",
    "    if batch:\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e21d0b-dba5-46e2-8f5d-fc21d2549bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY_CHAT\"))\n",
    "\n",
    "GPT5_NANO_MODEL = os.getenv(\"OPENAI_MODEL\")  # e.g. \"gpt-5-nano-2025-08-07\"\n",
    "\n",
    "\n",
    "def extract_entities_batch(rows_batch):\n",
    "    \"\"\"\n",
    "    rows_batch: list of namedtuples from df.itertuples()\n",
    "    Returns list of dicts:\n",
    "      {chunk_id, doc_id, entities, relations}\n",
    "    in the same order as rows_batch.\n",
    "    \"\"\"\n",
    "    if not rows_batch:\n",
    "        return []\n",
    "\n",
    "    # Build compact payload for the model\n",
    "    items = []\n",
    "    for r in rows_batch:\n",
    "        items.append(\n",
    "            {\n",
    "                \"chunk_id\": r.chunk_id,\n",
    "                \"doc_id\": r.doc_id,\n",
    "                \"text\": r.text or \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    system_msg = (\n",
    "        \"Your task is to process short text snippets from chunked documents,\\n\"\n",
    "        \"identifying key entities and relationships WITHOUT adding any external knowledge,\\n\"\n",
    "        \"assumptions, or hallucinations. Stick strictly to what is explicitly stated in the text.\\n\"\n",
    "        \"Extraction rules:\\n\"\n",
    "        \"1. Only extract entities that are clearly and directly mentioned.\\n\"\n",
    "        \"2. Use these entity types (aligned with investigations, legal proceedings, and networks):\\n\"\n",
    "        \"  PERSON: Named individuals, aliases, or explicitly identified roles\\n\"\n",
    "        \"          (victims, witnesses, accused, associates, officials).\\n\"\n",
    "        \"  ORG: Organizations, companies, foundations, institutions, law firms, airlines, etc.\\n\"\n",
    "        \"  LOCATION: Places, addresses, properties, geographic areas, residences, islands, cities.\\n\"\n",
    "        \"  EVENT: Explicit incidents, meetings, trips, legal actions, interrogations, flights, parties.\\n\"\n",
    "        \"  WORK: Documents, logs, books, media, lists (for example “flight logs”, “black book”).\\n\"\n",
    "        \"  DATE: Explicit dates, times, periods, years. Only extract literal time references.\\n\"\n",
    "        \"  OTHER: Only if necessary. Tail numbers, phone numbers, financial amounts.\\n\"\n",
    "        \"          Use sparingly and with justification.\\n\"\n",
    "        \"3. Relationships:\\n\"\n",
    "        \"  Only extract relationships that are explicitly implied by the text.\\n\"\n",
    "        \"  Do NOT infer anything or rely on outside knowledge.\\n\"\n",
    "        \"  Use descriptive predicates such as:\\n\"\n",
    "        \"    'associated_with'\\n\"\n",
    "        \"    'victim_of'\\n\"\n",
    "        \"    'accused_by'\\n\"\n",
    "        \"    'traveled_with'\\n\"\n",
    "        \"    'traveled_to'\\n\"\n",
    "        \"    'met_with'\\n\"\n",
    "        \"    'owned_by'\\n\"\n",
    "        \"    'mentioned_in'\\n\"\n",
    "        \"    'connected_to'\\n\"\n",
    "        \"  Only include a relationship if the text directly suggests the connection.\\n\"\n",
    "        \"You will receive a JSON array under the key 'items'. Each element has:\\n\"\n",
    "        \"  - chunk_id (string)\\n\"\n",
    "        \"  - doc_id (string or number)\\n\"\n",
    "        \"  - text (string)\\n\"\n",
    "        \"For each item, extract entities and relations.\\n\"\n",
    "        \"Return a JSON object with a single key 'results' that is an array.\\n\"\n",
    "        \"Each result must be in the same order as the input and have:\\n\"\n",
    "        \"  - chunk_id\\n\"\n",
    "        \"  - doc_id\\n\"\n",
    "        \"  - entities: list of { name, type }\\n\"\n",
    "        \"  - relations: list of { subject, predicate, object }\\n\"\n",
    "        \"Do NOT output anything outside the JSON object.\\n\"\n",
    "    )\n",
    "\n",
    "    user_payload = {\"items\": items}\n",
    "    user_msg = json.dumps(user_payload, ensure_ascii=False)\n",
    "\n",
    "    # Use chat.completions with JSON mode\n",
    "    resp = client.chat.completions.create(\n",
    "        model=GPT5_NANO_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    out_text = resp.choices[0].message.content\n",
    "    data = json.loads(out_text)\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    if len(results) != len(rows_batch):\n",
    "        raise ValueError(\n",
    "            f\"Model returned {len(results)} results for {len(rows_batch)} inputs\"\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a05d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting batched entity extraction on 2000 chunks\n",
      "Model: gpt-5-nano-2025-08-07\n",
      "Results file: entity_extraction_raw.jsonl\n",
      "Failed file:  entity_extraction_failed.jsonl\n",
      "================================================================================\n",
      "[Batch 1] Error: Model returned 18 results for 200 inputs\n",
      "[Batch 2] Error: Model returned 25 results for 200 inputs\n",
      "[Batch 3] Error: Model returned 0 results for 200 inputs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m end_row = start_row + batch_size - \u001b[32m1\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     batch_results = \u001b[43mextract_entities_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Write one line per chunk, same format as before\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m batch_results:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mextract_entities_batch\u001b[39m\u001b[34m(rows_batch)\u001b[39m\n\u001b[32m     74\u001b[39m user_msg = json.dumps(user_payload, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Use chat.completions with JSON mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGPT5_NANO_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m out_text = resp.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     87\u001b[39m data = json.loads(out_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\temp\\chunk\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1234\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1231\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1232\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1233\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1107\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from time import sleep, time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIG FOR MONITORING ----------------\n",
    "\n",
    "MAX_ROWS_FOR_RUN = len(df)      # or len(df) when ready\n",
    "PROGRESS_EVERY_BATCH = 5        # print every N batches\n",
    "SAVE_EVERY_BATCH = 5            # flush every N batches\n",
    "SLEEP_BETWEEN_BATCHES = 0       # set to small number if you hit RPM limits\n",
    "\n",
    "results_path = \"entity_extraction_raw.jsonl\"\n",
    "failed_path = \"entity_extraction_failed.jsonl\"\n",
    "\n",
    "truncate_outputs = True\n",
    "\n",
    "if truncate_outputs:\n",
    "    open(results_path, \"w\", encoding=\"utf-8\").close()\n",
    "    open(failed_path, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "df_subset = df.iloc[:MAX_ROWS_FOR_RUN].copy()\n",
    "total_rows = len(df_subset)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Starting batched entity extraction on {total_rows} chunks\")\n",
    "print(f\"Model: {GPT5_NANO_MODEL}\")\n",
    "print(f\"Results file: {results_path}\")\n",
    "print(f\"Failed file:  {failed_path}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "success_rows = 0\n",
    "failed_rows = 0\n",
    "batch_index = 0\n",
    "start_time = time()\n",
    "\n",
    "with open(results_path, \"a\", encoding=\"utf-8\") as f_ok, \\\n",
    "     open(failed_path, \"a\", encoding=\"utf-8\") as f_fail:\n",
    "\n",
    "    for batch_rows in iter_token_batches(df_subset):\n",
    "        batch_index += 1\n",
    "        batch_size = len(batch_rows)\n",
    "        start_row = success_rows + failed_rows + 1\n",
    "        end_row = start_row + batch_size - 1\n",
    "\n",
    "        try:\n",
    "            batch_results = extract_entities_batch(batch_rows)\n",
    "\n",
    "            # Write one line per chunk, same format as before\n",
    "            for r in batch_results:\n",
    "                out = {\n",
    "                    \"chunk_id\": r.get(\"chunk_id\"),\n",
    "                    \"doc_id\": r.get(\"doc_id\"),\n",
    "                    \"entities\": r.get(\"entities\", []),\n",
    "                    \"relations\": r.get(\"relations\", []),\n",
    "                }\n",
    "                f_ok.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            success_rows += batch_size\n",
    "\n",
    "        except Exception as e:\n",
    "            # If the whole batch fails, record all rows in failed file\n",
    "            print(f\"[Batch {batch_index}] Error: {e}\")\n",
    "            for row in batch_rows:\n",
    "                err = {\n",
    "                    \"chunk_id\": row.chunk_id,\n",
    "                    \"doc_id\": row.doc_id,\n",
    "                    \"error\": str(e),\n",
    "                }\n",
    "                f_fail.write(json.dumps(err, ensure_ascii=False) + \"\\n\")\n",
    "            failed_rows += batch_size\n",
    "\n",
    "        # Progress and ETA\n",
    "        if batch_index % PROGRESS_EVERY_BATCH == 0 or success_rows + failed_rows >= total_rows:\n",
    "            done = success_rows + failed_rows\n",
    "            elapsed = time() - start_time\n",
    "            rate = done / elapsed if elapsed > 0 else 0\n",
    "            rows_per_min = rate * 60 if rate > 0 else 0\n",
    "            remaining = total_rows - done\n",
    "            eta_sec = remaining / rate if rate > 0 else 0\n",
    "            pct = done / total_rows * 100\n",
    "\n",
    "            print(\n",
    "                f\"[Batch {batch_index:4d}] rows {done:6d}/{total_rows:6d} \"\n",
    "                f\"({pct:5.1f}%) succ={success_rows:6d} fail={failed_rows:5d} \"\n",
    "                f\"elapsed={elapsed/60:5.1f}m eta={eta_sec/60:5.1f}m \"\n",
    "                f\"rows/min={rows_per_min:6.1f}\"\n",
    "            )\n",
    "\n",
    "        if batch_index % SAVE_EVERY_BATCH == 0:\n",
    "            f_ok.flush()\n",
    "            f_fail.flush()\n",
    "            print(\n",
    "                f\"  Flushed at batch {batch_index}. \"\n",
    "                f\"Success rows={success_rows}, Failed rows={failed_rows}\"\n",
    "            )\n",
    "\n",
    "        if SLEEP_BETWEEN_BATCHES > 0:\n",
    "            sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "elapsed_total = time() - start_time\n",
    "print(\"=\" * 80)\n",
    "print(\"Done batched entity extraction loop\")\n",
    "print(f\"Success rows: {success_rows}, Failed rows: {failed_rows}\")\n",
    "print(f\"Total time: {elapsed_total/60:5.1f} minutes\")\n",
    "print(f\"Final results in {results_path}\")\n",
    "if failed_rows > 0:\n",
    "    print(f\"Failed rows in {failed_path}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame of entity mentions (one row per mention)\n",
    "mention_rows = []\n",
    "\n",
    "for r in results:\n",
    "    chunk_id = r[\"chunk_id\"]\n",
    "    doc_id = r[\"doc_id\"]\n",
    "    ents = r[\"entities\"]\n",
    "    for e in ents:\n",
    "        name = (e.get(\"name\") or \"\").strip()\n",
    "        etype = (e.get(\"type\") or \"OTHER\").upper()\n",
    "        if not name:\n",
    "            continue\n",
    "        mention_rows.append(\n",
    "            {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"name\": name,\n",
    "                \"type\": etype,\n",
    "            }\n",
    "        )\n",
    "\n",
    "mentions_df = pd.DataFrame(mention_rows)\n",
    "print(\"Mention rows:\", len(mentions_df))\n",
    "mentions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d31fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name: str) -> str:\n",
    "    n = name.strip()\n",
    "    n = \" \".join(n.split())\n",
    "    return n\n",
    "\n",
    "mentions_df[\"norm_name\"] = mentions_df[\"name\"].apply(normalize_name)\n",
    "\n",
    "# group by normalized name + type\n",
    "grouped = mentions_df.groupby([\"norm_name\", \"type\"])\n",
    "\n",
    "entity_rows = []\n",
    "entity_id_map: Dict[Tuple[str, str], int] = {}\n",
    "next_eid = 1\n",
    "\n",
    "for (norm_name, etype), group in grouped:\n",
    "    eid = next_eid\n",
    "    entity_id_map[(norm_name, etype)] = eid\n",
    "    next_eid += 1\n",
    "\n",
    "    doc_ids = sorted(set(group[\"doc_id\"].dropna().tolist()))\n",
    "    chunk_ids = sorted(set(group[\"chunk_id\"].dropna().tolist()))\n",
    "    freq = len(group)\n",
    "\n",
    "    entity_rows.append(\n",
    "        {\n",
    "            \"entity_id\": eid,\n",
    "            \"name\": norm_name,\n",
    "            \"type\": etype,\n",
    "            \"frequency\": freq,\n",
    "            \"doc_ids\": json.dumps(doc_ids),\n",
    "            \"chunk_ids\": json.dumps(chunk_ids),\n",
    "        }\n",
    "    )\n",
    "\n",
    "entities_df = pd.DataFrame(entity_rows)\n",
    "print(\"Canonical entities:\", len(entities_df))\n",
    "entities_df.head()\n",
    "\n",
    "def get_eid(row):\n",
    "    key = (row[\"norm_name\"], row[\"type\"])\n",
    "    return entity_id_map.get(key)\n",
    "\n",
    "mentions_df[\"entity_id\"] = mentions_df.apply(get_eid, axis=1)\n",
    "mentions_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk -> Entity edges\n",
    "mention_edges = []\n",
    "for _, row in mentions_df.iterrows():\n",
    "    mention_edges.append(\n",
    "        (\"chunk:\" + str(row[\"chunk_id\"]), \"MENTIONS\", \"entity:\" + str(row[\"entity_id\"]))\n",
    "    )\n",
    "\n",
    "len(mention_edges)\n",
    "# Entity co-occurrence edges based on chunk co-appearance\n",
    "\n",
    "co_counts = Counter()\n",
    "\n",
    "for chunk_id, grp in mentions_df.groupby(\"chunk_id\"):\n",
    "    eids = sorted(set(grp[\"entity_id\"].dropna().tolist()))\n",
    "    for i in range(len(eids)):\n",
    "        for j in range(i + 1, len(eids)):\n",
    "            a, b = eids[i], eids[j]\n",
    "            key = (a, b)\n",
    "            co_counts[key] += 1\n",
    "\n",
    "co_rows = []\n",
    "for (a, b), w in co_counts.items():\n",
    "    co_rows.append(\n",
    "        {\n",
    "            \"source\": \"entity:\" + str(a),\n",
    "            \"target\": \"entity:\" + str(b),\n",
    "            \"relation\": \"RELATED_TO\",\n",
    "            \"weight\": w,\n",
    "        }\n",
    "    )\n",
    "\n",
    "edges_df = pd.DataFrame(co_rows)\n",
    "print(\"Entity-entity edges:\", len(edges_df))\n",
    "edges_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_rows = []\n",
    "\n",
    "for r in results:\n",
    "    chunk_id = r[\"chunk_id\"]\n",
    "    rels = r[\"relations\"]\n",
    "    ents = r[\"entities\"]\n",
    "\n",
    "    # build map from name -> (norm, type, eid)\n",
    "    local_map = {}\n",
    "    for e in ents:\n",
    "        name = (e.get(\"name\") or \"\").strip()\n",
    "        etype = (e.get(\"type\") or \"OTHER\").upper()\n",
    "        if not name:\n",
    "            continue\n",
    "        norm = normalize_name(name)\n",
    "        eid = entity_id_map.get((norm, etype))\n",
    "        if eid:\n",
    "            local_map[name] = eid\n",
    "\n",
    "    for rel in rels:\n",
    "        subj = rel.get(\"subject\")\n",
    "        obj = rel.get(\"object\")\n",
    "        pred = rel.get(\"predicate\") or \"connected_to\"\n",
    "\n",
    "        if not subj or not obj:\n",
    "            continue\n",
    "        eid_s = local_map.get(subj)\n",
    "        eid_o = local_map.get(obj)\n",
    "        if not eid_s or not eid_o:\n",
    "            continue\n",
    "\n",
    "        relation_rows.append(\n",
    "            {\n",
    "                \"source\": \"entity:\" + str(eid_s),\n",
    "                \"target\": \"entity:\" + str(eid_o),\n",
    "                \"relation\": pred,\n",
    "                \"weight\": 1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "explicit_rel_df = pd.DataFrame(relation_rows)\n",
    "print(\"Explicit relations edges:\", len(explicit_rel_df))\n",
    "explicit_rel_df.head()\n",
    "\n",
    "all_edges_df = pd.concat([edges_df, explicit_rel_df], ignore_index=True)\n",
    "print(\"Total edges:\", len(all_edges_df))\n",
    "all_edges_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df.to_csv(\"entities.csv\", index=False)\n",
    "all_edges_df.to_csv(\"edges.csv\", index=False)\n",
    "print(\"Wrote entities.csv and edges.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
